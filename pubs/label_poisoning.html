<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" href="../main.css">
        <link rel="stylesheet" href="../content.css">
        <link rel="shortcut icon" type="image/png" href="../im/favicon.png"/>
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Abril+Fatface&family=Lato:wght@300;400;700&display=swap" rel="stylesheet">
        <title>Rishi Jha</title>
    </head>
    <body>
        <div class="outer">
            <main>
                <h1><a href="../index.html">Rishi D. Jha</a></h1>
                <div class="sidebar">
                    <a class="sidebar_content" href="../publications.html">publications</a>
                </div>
                <b class="line"></b>
                <div class="content">
                    <p class="ptitle">Label Poisoning is All You Need</p>
                    <p class="subtitle"> &#x2022; <b>NeurIPS '23</b> | Rishi Jha, Jonathan Hayase, Sewoong Oh</p>
                    <p class="subtitle" style="padding-top: 0;"> &#x2022; <b>Master's Thesis</b> | Rishi Jha</p>
                    <p class="links"><a class="links" href="https://neurips.cc/virtual/2023/poster/70392">[conference]</a> <a class="links" href="../rishi_jha_thesis.pdf">[thesis]</a> <a class="links" href="https://github.com/SewoongLab/FLIP">[code]</a></p>
                    <p class="abstract">
                        In a backdoor attack, an adversary injects corrupted data into a model's training dataset in order to gain control over its predictions on images with a specific attacker-defined trigger.
                        A typical corrupted training example requires altering both the image, by applying the trigger, and the label.
                        Models trained on clean images, therefore, were considered safe from backdoor attacks.
                        However, in some common machine learning scenarios, the training labels are provided by potentially malicious third-parties.
                        This includes crowd-sourced annotation and knowledge distillation.
                        We, hence, investigate a fundamental question: can we launch a successful backdoor attack by only corrupting labels? We introduce a novel approach to design label-only backdoor attacks, which we call FLIP, and demonstrate its strengths on three datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) and four architectures (ResNet-32, ResNet-18, VGG-19, and Vision Transformer).
                        With only 2% of CIFAR-10 labels corrupted, FLIP achieves a near-perfect attack success rate of 99.4% while suffering only a 1.8% drop in the clean test accuracy.
                        Our approach builds upon the recent advances in trajectory matching, originally introduced for dataset distillation.
                    </p>
                    <br>
                    <img src="../im/poster.png" alt="FLIP Poster Diagram">
                </div>
            </main>        
        </div>
    </body>
</html>